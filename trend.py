# Googleãƒˆãƒ¬ãƒ³ãƒ‰APIç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from pytrends.request import TrendReq
# æ­£è¦è¡¨ç¾ã‚’ä½¿ã†ãŸã‚
import re
# è¾æ›¸å‹ã§åˆæœŸå€¤0ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½œã‚‹ãŸã‚
from collections import defaultdict
# å‡¦ç†ã‚’ä¸€æ™‚åœæ­¢ã•ã›ã‚‹ãŸã‚
import time
# ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ã‚’ä½œã‚‹ãŸã‚
import random

# ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ»æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å—ã‘å–ã‚Šã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‹ã‚µãƒ–ï¼‰ã‚’è¿”ã™é–¢æ•°
def extract_main_and_sub_related(user_id: str, user_input: str, max_results=10):
    try:
        # å…¥åŠ›ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        query = user_input.strip()

        # Googleå´ã®åˆ¶é™å›é¿ã®ãŸã‚ã€6ã€œ10ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
        time.sleep(random.uniform(6, 10))  

        # Googleãƒˆãƒ¬ãƒ³ãƒ‰ã¸ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æº–å‚™
        pytrends = TrendReq(hl='ja-JP', tz=540)  # æ—¥æœ¬èªãƒ»æ—¥æœ¬æ™‚é–“ã§è¨­å®š

        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆï¼ˆéå»1æ—¥ã®æ—¥æœ¬å›½å†…ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
        pytrends.build_payload([query], geo='JP', timeframe='now 1-d')

        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—ï¼ˆå¤±æ•—ã—ãŸã‚‰æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
        for attempt in range(3):
            try:
                related = pytrends.related_queries()
                break
            except Exception as e:
                # 1å›ç›®â†’15ç§’å¾…æ©Ÿã€2å›ç›®â†’25ç§’å¾…æ©Ÿ
                if attempt < 2:
                    wait = 15 + attempt * 10
                    print(f"429 or other error, retrying in {wait}s...")
                    time.sleep(wait)
                else:
                    # 3å›å¤±æ•—ã—ãŸã‚‰ã‚¨ãƒ©ãƒ¼ã‚’ãã®ã¾ã¾è¿”ã™
                    raise e

        # ã€Œæ€¥ä¸Šæ˜‡ã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = related.get(query, {}).get('rising')
        if df is None or df.empty:
            return "é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        # å˜èªã”ã¨ã®ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
        word_scores = defaultdict(int)
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡Œã‚’ä¿å­˜ï¼ˆå¾Œã§ã‚µãƒ–é–¢é€£èªæŠ½å‡ºã«ä½¿ã†ï¼‰
        original_rows = []
        # æ—¢ã«ä½¿ã£ãŸå˜èªã‚’è¨˜éŒ²ï¼ˆæœ€åˆã‹ã‚‰æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã¯ä½¿ç”¨æ¸ˆã¿ã«ï¼‰
        used_words = set([query])

        # -------------------------------
        # ãƒ¡ã‚¤ãƒ³é–¢é€£èªå€™è£œã‚’ã‚¹ã‚³ã‚¢é›†è¨ˆ
        # -------------------------------
        for row in df.itertuples():
            full_query = row.query  # é–¢é€£ãƒ¯ãƒ¼ãƒ‰å…¨ä½“
            score = int(row.value)  # äººæ°—åº¦ã‚¹ã‚³ã‚¢
            original_rows.append((full_query, score))

            # å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¦æ®‹ã£ãŸéƒ¨åˆ†ã‚’å˜èªã”ã¨ã«åˆ†å‰²
            cleaned_full_query = full_query.replace(query, '').strip()
            words = [w for w in re.split(r'\s+', cleaned_full_query) if len(w) > 1]

            # ã¾ã ä½¿ã£ã¦ã„ãªã„å˜èªãªã‚‰ã‚¹ã‚³ã‚¢ã‚’åŠ ç®—
            for w in words:
                if w not in used_words:
                    word_scores[w] += score

        # ã‚¹ã‚³ã‚¢é †ã«ä¸¦ã¹æ›¿ãˆï¼ˆé«˜ã„é †ï¼‰
        sorted_main = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        results = []

        # -------------------------------
        # ãƒ¡ã‚¤ãƒ³é–¢é€£èªã¨ã€ãã®ã‚µãƒ–é–¢é€£èªã‚’æŠ½å‡º
        # -------------------------------
        for main_word, main_score in sorted_main:
            # æ—¢ã«ä½¿ã£ãŸå˜èªã¯ã‚¹ã‚­ãƒƒãƒ—
            if main_word in used_words:
                continue  

            sub_words = []  # ã‚µãƒ–é–¢é€£èª
            sub_candidates = []  # å€™è£œç”¨ï¼ˆæœªä½¿ç”¨ã ãŒä»Šå›ã¯ä½¿ã£ã¦ãªã„ï¼‰

            # äººæ°—åº¦ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³ã‚’ä»˜ä¸
            if main_score >= 1000:
                score_icon = "ğŸ”´"  # è¶…äººæ°—
            elif main_score >= 100:
                score_icon = "ğŸŸ "  # äººæ°—
            elif main_score >= 10:
                score_icon = "ğŸŸ¡"  # å°‘ã—äººæ°—
            else:
                score_icon = ""    # ç„¡å°

            # å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ–é–¢é€£èªã‚’æ¢ã™
            for full_query, score in original_rows:
                # ãƒ¡ã‚¤ãƒ³èªã¨æ¤œç´¢èªãŒä¸¡æ–¹å«ã¾ã‚Œã¦ã„ã‚‹è¡Œã ã‘ã‚’è¦‹ã‚‹
                if main_word in full_query and query in full_query:
                    # ãƒ¡ã‚¤ãƒ³èªã¨æ¤œç´¢èªã‚’å‰Šé™¤
                    cleaned = re.sub(query, '', full_query)
                    cleaned = re.sub(main_word, '', cleaned)

                    # æ®‹ã£ãŸå˜èªã‚’å€™è£œã«
                    candidates = [w.strip() for w in cleaned.split() if w.strip() and w != query and w != main_word]

                    # æœªä½¿ç”¨ãƒ»é•·ã•2æ–‡å­—ä»¥ä¸Šãƒ»ã¾ã è¿½åŠ ã—ã¦ãªã„å˜èªã ã‘ã‚’ã‚µãƒ–èªã«è¿½åŠ 
                    for w in candidates:
                        if w not in used_words and len(w) > 1 and w not in sub_words:
                            sub_words.append(w)
                            if len(sub_words) >= 3:  # ã‚µãƒ–èªã¯æœ€å¤§3ã¤
                                break

            # ãƒ¡ã‚¤ãƒ³èªã¨ã‚µãƒ–èªã‚’ä½¿ç”¨æ¸ˆã¿ã«ç™»éŒ²
            used_words.add(main_word)
            used_words.update(sub_words)

            # ã‚µãƒ–é–¢é€£èªãŒãªã‘ã‚Œã°ã€Œãªã—ã€
            sub_str = "ã€".join(sub_words) if sub_words else "ãªã—"
            results.append(f"{score_icon}{main_word}(+{main_score}%)\nâ”—ï½»ï¾Œï¾é–¢é€£:{sub_str}")

            # ä¸Šé™æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
            if len(results) >= max_results:
                break

            # æ¬¡ã®å‡¦ç†å‰ã«0.5ã€œ1.2ç§’ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿï¼ˆGoogleå´è² è·è»½æ¸›ï¼‰
            time.sleep(random.uniform(0.5, 1.2))

        # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
        return "\n".join(results)

    except Exception as e:
        import traceback
        # ä¾‹å¤–å†…å®¹ã‚’æ–‡å­—åˆ—ã§è¿”ã™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š\n{traceback.format_exc()}"
