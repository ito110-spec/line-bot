from pytrends.request import TrendReq
import re
from collections import defaultdict
import time
import random
import urllib.parse

pytrends = TrendReq(hl='ja-JP', tz=540)

def make_news_link(base_query: str, main_word: str, sub_word: str):
    q = f"{base_query} {main_word} {sub_word}"
    q_encoded = urllib.parse.quote(q)
    return f"https://news.google.com/search?q={q_encoded}&hl=ja&gl=JP&ceid=JP:ja"

def extract_main_and_sub_related(user_input: str, max_results=10):
    try:
        query = user_input.strip()

        # â˜… Googleãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã«å¼·åˆ¶ã‚¦ã‚§ã‚¤ãƒˆï¼ˆ6ï½10ç§’ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        time.sleep(random.uniform(6, 10))

        # â˜… build_payloadï¼ˆã“ã‚Œã¯åŸºæœ¬çš„ã«OKï¼‰
        pytrends.build_payload([query], geo='JP', timeframe='now 1-d')

        # â˜… related_queries ã«ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§ã‚¢ã‚¯ã‚»ã‚¹
        for attempt in range(3):
            try:
                related = pytrends.related_queries()
                break
            except Exception as e:
                if attempt < 2:
                    wait = 15 + attempt * 10
                    print(f"429 or other error, retrying in {wait}s...")
                    time.sleep(wait)
                else:
                    raise e  # æœ€çµ‚çš„ã«å¤±æ•—

        df = related.get(query, {}).get('rising')

        if df is None or df.empty:
            return "é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        word_scores = defaultdict(int)
        original_rows = []

        for row in df.itertuples():
            full_query = row.query
            score = int(row.value)
            original_rows.append((full_query, score))

            cleaned_full_query = full_query.replace(query, '').strip()
            words = [w for w in re.split(r'\s+', cleaned_full_query) if len(w) > 1]
            for w in words:
                word_scores[w] += score

        sorted_main = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        results = []

        for main_word, main_score in sorted_main[:max_results]:
            sub_links = []

            # ğŸ”´ğŸŸ ğŸŸ¡ï¼šã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
            if main_score >= 1000:
                score_icon = "ğŸ”´"
            elif main_score >= 100:
                score_icon = "ğŸŸ "
            elif main_score >= 10:
                score_icon = "ğŸŸ¡"
            else:
                score_icon = ""

            for full_query, score in original_rows:
                if main_word in full_query and query in full_query:
                    cleaned = re.sub(query, '', full_query)
                    cleaned = re.sub(main_word, '', cleaned)
                    sub_parts = [w.strip() for w in cleaned.split() if w.strip() and w != main_word and w != query]

                    for w in sub_parts:
                        if w not in sub_links and len(w) > 1:
                            link = make_news_link(query, main_word, w)
                            sub_links.append(f"[{w}]({link})")
                            if len(sub_links) >= 3:
                                break

            sub_str = "ã€".join(sub_links) if sub_links else "ãªã—"
            results.append(f"{score_icon}{main_word}(+{main_score}%)\nâ”—ï½»ï¾Œï¾é–¢é€£:{sub_str}")

            # çµæœç”Ÿæˆå´ã«ã‚‚è»½ã‚ã®ã‚¦ã‚§ã‚¤ãƒˆ
            time.sleep(random.uniform(0.5, 1.2))

        return "\n".join(results)

    except Exception as e:
        import traceback
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š\n{traceback.format_exc()}"
